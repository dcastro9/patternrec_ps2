\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{Bache+Lichman:2013}
\citation{yagnik2011power}
\@LN@col{1}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Alcoholism Dataset}{1}{section.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Description}{1}{subsection.2.1}}
\@LN@col{2}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Attempted Methods}{1}{subsection.2.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Results}{1}{subsection.2.3}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.3.1}Original Dataset}{1}{subsubsection.2.3.1}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Original Dataset - Best Results with k-Value\relax }}{1}{table.caption.2}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{alc_o_res_table}{{1}{1}{Original Dataset - Best Results with k-Value\relax }{table.caption.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces 50\% of the TD. In this we see great stability and it is easier to view a trend based on the given k values.\relax }}{1}{figure.caption.3}}
\@LN@col{1}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces 80\% of the TD. Here we see the greatest stability with 5-fold and N-fold CV converging to very similar classification results. 2-fold CV approaches a similar shape but at less accuracy.\relax }}{2}{figure.caption.4}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.3.2}Normalized Dataset}{2}{subsubsection.2.3.2}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Normalized Dataset - Best Results with k-Value\relax }}{2}{table.caption.5}}
\newlabel{alc_n_res_table}{{2}{2}{Normalized Dataset - Best Results with k-Value\relax }{table.caption.5}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.3.3}Normalized \& Weighted Dataset}{2}{subsubsection.2.3.3}}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces Normalized \& Weighted Dataset - Best Results with k-Value\relax }}{2}{table.caption.7}}
\newlabel{alc_nw_res_table}{{3}{2}{Normalized \& Weighted Dataset - Best Results with k-Value\relax }{table.caption.7}{}}
\@LN@col{2}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces 50\% of the TD. We see an interesting peak at approximately 7 neighbors, with a performance of 66.6\% for N-Fold validation.\relax }}{2}{figure.caption.6}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces 80\% of the TD. It is interesting to see that 5-fold and N-fold cross validation follow a similar trend, while 2-fold cross validation approaches their results as k becomes larger.\relax }}{2}{figure.caption.8}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.3.4}Whitened Dataset}{2}{subsubsection.2.3.4}}
\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces Whitened Dataset - Best Results with k-Value\relax }}{2}{table.caption.9}}
\newlabel{alc_w_res_table}{{4}{2}{Whitened Dataset - Best Results with k-Value\relax }{table.caption.9}{}}
\@LN@col{1}
\@writefile{toc}{\contentsline {section}{\numberline {3}Bank-Note Authentication Dataset}{3}{section.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Description}{3}{subsection.3.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Attempted Methods}{3}{subsection.3.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Results}{3}{subsection.3.3}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.1}Original Dataset}{3}{subsubsection.3.3.1}}
\@writefile{lot}{\contentsline {table}{\numberline {5}{\ignorespaces Original Dataset - Best Results with k-Value\relax }}{3}{table.caption.10}}
\newlabel{dba_n_res_table}{{5}{3}{Original Dataset - Best Results with k-Value\relax }{table.caption.10}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.2}Whitened Dataset}{3}{subsubsection.3.3.2}}
\@writefile{lot}{\contentsline {table}{\numberline {6}{\ignorespaces Whitened Dataset - Best Results with k-Value\relax }}{3}{table.caption.11}}
\newlabel{dba_w_res_table}{{6}{3}{Whitened Dataset - Best Results with k-Value\relax }{table.caption.11}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Diabetes in Pima Indians Dataset}{3}{section.4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Description}{3}{subsection.4.1}}
\@LN@col{2}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces 80\% of the TD. The dataset is fairly accurate with one neighbor regardless of the Cross Validation technique implemented.\relax }}{3}{figure.caption.12}}
\newlabel{fig:dbaw}{{5}{3}{80\% of the TD. The dataset is fairly accurate with one neighbor regardless of the Cross Validation technique implemented.\relax }{figure.caption.12}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Attempted Methods}{3}{subsection.4.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Results}{3}{subsection.4.3}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.3.1}Original Dataset}{3}{subsubsection.4.3.1}}
\@writefile{lot}{\contentsline {table}{\numberline {7}{\ignorespaces Original Dataset - Best Results with k-Value\relax }}{3}{table.caption.13}}
\newlabel{dba_n_res_table}{{7}{3}{Original Dataset - Best Results with k-Value\relax }{table.caption.13}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.3.2}Whitened Dataset}{3}{subsubsection.4.3.2}}
\@writefile{lot}{\contentsline {table}{\numberline {8}{\ignorespaces Whitened Dataset - Best Results with k-Value\relax }}{3}{table.caption.15}}
\newlabel{dba_w_res_table}{{8}{3}{Whitened Dataset - Best Results with k-Value\relax }{table.caption.15}{}}
\bibstyle{acmsiggraph}
\bibdata{template}
\bibcite{Bache+Lichman:2013}{\citename {Bache and Lichman }2013}
\bibcite{yagnik2011power}{\citename {Yagnik et\nobreakspace  {}al\unhbox \voidb@x \hbox {.} }2011}
\@LN@col{1}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces 80\% of the TD. Here we can see how the cross validation results reach a peak at approximately k=10 and stop increasing in accuracy as k is increased. For 2-fold validation, the increase is seen a bit earlier.\relax }}{4}{figure.caption.14}}
\newlabel{fig:pio}{{6}{4}{80\% of the TD. Here we can see how the cross validation results reach a peak at approximately k=10 and stop increasing in accuracy as k is increased. For 2-fold validation, the increase is seen a bit earlier.\relax }{figure.caption.14}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces 80\% of the TD. We see a similar trend here as we see in Figure \ref  {fig:pio}, demonstrating the negligible effect of whitening in this scenario. \relax }}{4}{figure.caption.16}}
\newlabel{fig:piw}{{7}{4}{80\% of the TD. We see a similar trend here as we see in Figure \ref {fig:pio}, demonstrating the negligible effect of whitening in this scenario. \relax }{figure.caption.16}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Conclusion}{4}{section.5}}
\@LN@col{2}
\@LN@col{1}
\@writefile{toc}{\contentsline {section}{\numberline {6}Remaining Figures}{5}{section.6}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces 20\% of the TD. We see little to no correlation in the data here, which is expected given that you are randomly selecting 20 percent of the data, these training sets were unstable.\relax }}{5}{figure.caption.19}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces 100\% of the TD. This becomes a lot more volatile because we are not running five iterations of a random subset (since this subset spans the entire set). Albeit the volatility, this achieves the highest accuracy with 7 nearest neighbors.\relax }}{5}{figure.caption.20}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces 20\% of the TD. We see a slight increase up until approximately 15 nearest neighbors, but its hard to spot a trend. N-Fold validation outperforms the other CV methods at 19 nearest neighbors.\relax }}{5}{figure.caption.21}}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces 80\% of the TD. It is difficult to discern a trend in this visualization which we attribute to the granularity of the axes, the spikes are potentially due to noise in obtaining a random 80\% subset of the training data.\relax }}{5}{figure.caption.22}}
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces 100\% of the TD. Here we see a trend with all methods of cross validation, although there is a significant amount of noise between 61 and 65\% accuracy that is hard to attribute to a change in k.\relax }}{5}{figure.caption.23}}
\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces 20\% of the TD. We see the data follow a similar trend and reach a peak around 5-10 k values, and then dropping down, with 2-fold having the largest instability.\relax }}{6}{figure.caption.24}}
\@writefile{lof}{\contentsline {figure}{\numberline {14}{\ignorespaces 50\% of the TD. Here the cross validation methods follow a similar trend, increase towards k=13.\relax }}{6}{figure.caption.25}}
\@writefile{lof}{\contentsline {figure}{\numberline {15}{\ignorespaces 100\% of the TD. At 100\% we see little stability due to the fact that the results are run only once and are therefore not an average of five iterations of a random subset.\relax }}{6}{figure.caption.26}}
\@writefile{lof}{\contentsline {figure}{\numberline {16}{\ignorespaces 20\% of the TD. We see little to no correlation in the data here, which is expected given that you are randomly selecting 20 percent of the data, these training sets were unstable.\relax }}{6}{figure.caption.27}}
\@writefile{lof}{\contentsline {figure}{\numberline {17}{\ignorespaces 50\% of the TD. In this we see great stability and it is easier to view a trend based on the given k values.\relax }}{6}{figure.caption.28}}
\@writefile{lof}{\contentsline {figure}{\numberline {18}{\ignorespaces 80\% of the TD. Here we see the greatest stability with 5-fold and N-fold CV converging to very similar classification results. 2-fold CV approaches a similar shape but at less accuracy.\relax }}{6}{figure.caption.29}}
\@writefile{lof}{\contentsline {figure}{\numberline {19}{\ignorespaces 100\% of the TD. This becomes a lot more volatile because we are not running five iterations of a random subset (since this subset spans the entire set). Albeit the volatility, this achieves the highest accuracy with 7 nearest neighbors.\relax }}{7}{figure.caption.30}}
\@LN@col{2}
